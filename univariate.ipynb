{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9dd40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process the Excel file\n",
    "def process_excel_file(file_path):\n",
    "    try:\n",
    "        # Read the Excel file into a pandas DataFrame\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "        \n",
    "        df['TimeSeries_Key'] = df['product_code'] + '_' + df['customer_code']\n",
    "\n",
    "        df['Week'] = df['calweek'].astype(str).str[-2:].astype(int)\n",
    "\n",
    "        #df.columns\n",
    "\n",
    "        df1 = df.drop(['Cluster/Country/Region', 'SubCluster', 'brand', 'dc'], axis = 1)\n",
    "\n",
    "        timeseries_key_values = list(df1['TimeSeries_Key'].unique())\n",
    "\n",
    "    \n",
    "        start_week = 16\n",
    "        end_week = 28\n",
    "\n",
    "        filtered_df = df[(df['Week'] >= start_week) & (df['Week'] <= end_week)]\n",
    "\n",
    "        def generate_dataframes():\n",
    "            grouped_df = filtered_df.groupby('TimeSeries_Key')\n",
    "            for key, group in grouped_df:\n",
    "                group['TimeSeries_Key'] = key\n",
    "                yield group\n",
    "\n",
    "        combined_df = pd.concat(generate_dataframes(), ignore_index=True)\n",
    "        combined_df = combined_df.drop_duplicates(subset=['TimeSeries_Key', 'Week'])\n",
    "        combined_df = combined_df.drop([\"product_code\", \"customer_code\", \"Quantity\", \"Week\", \"calweek\"], axis = 1)\n",
    "        #combined_df\n",
    "        \n",
    "        df_prod1 = df1[df1['TimeSeries_Key']=='PRDCT1_CUSTA']\n",
    "        #df_prod1\n",
    "        # Group data by 'calweek' and aggregate 'Quantity'\n",
    "        ts_data = df_prod1.groupby('calweek')['Quantity'].sum()\n",
    "        #ts_data\n",
    "        \n",
    "#         from statsmodels.tsa.stattools import adfuller\n",
    "#         passing_data=adfuller(ts_data)\n",
    "#         def adf_test(sales):\n",
    "#             result=adfuller(ts_data)\n",
    "#             labels = ['Test parameters', 'p-value','#Lags Used','Dataset observations']\n",
    "#             for value,label in zip(result,labels):\n",
    "#                 print(label+' : '+str(value) )\n",
    "#             if result[1] <= 0.05:\n",
    "#                 print(\"Dataset is stationary\")\n",
    "#             else:\n",
    "#                 print(\"Dataset is non-stationary \")\n",
    "#         adf_test(ts_data)\n",
    "\n",
    "        # Create an empty dataframe to store MAPE values\n",
    "        results_df = pd.DataFrame()\n",
    "        results_mape = pd.DataFrame()\n",
    "        \n",
    "        # ARIMA model\n",
    "        def arima_forecast(train, test):\n",
    "\n",
    "            # Find optimal p, d, q values using Auto ARIMA\n",
    "            auto_arima_model = pm.auto_arima(train, seasonal=False, suppress_warnings=True)\n",
    "\n",
    "            p, d, q = auto_arima_model.order\n",
    "            model = ARIMA(train, order=(p, d, q))  # ARIMA(p, d, q)\n",
    "            model_fit = model.fit()\n",
    "            forecast = model_fit.forecast(steps=len(test))\n",
    "            return forecast\n",
    "\n",
    "        # SARIMA model\n",
    "        def sarima_forecast(train, test):\n",
    "\n",
    "            # Find optimal p, d, q, P, D, Q, and m values using Auto SARIMA\n",
    "            auto_sarima_model = pm.auto_arima(train, seasonal=True, m=12, suppress_warnings=True)\n",
    "            p, d, q = auto_sarima_model.order\n",
    "            P, D, Q, m = auto_sarima_model.seasonal_order\n",
    "\n",
    "\n",
    "            model = SARIMAX(train, order=(p, d, q), seasonal_order=(P, D, Q, m))  # SARIMA(p, d, q)(P, D, Q, m)\n",
    "            model_fit = model.fit()\n",
    "            forecast = model_fit.forecast(steps=len(test))\n",
    "            return forecast\n",
    "\n",
    "        # Holt-Winters model\n",
    "        def holt_winters_forecast(train, test):\n",
    "            model = ExponentialSmoothing(train, seasonal_periods=12, trend='add', seasonal='add')\n",
    "            model_fit = model.fit()\n",
    "            forecast = model_fit.forecast(len(test))\n",
    "            return forecast\n",
    "\n",
    "        # Facebook Prophet model\n",
    "        def prophet_forecast(train, test):\n",
    "            model = Prophet()\n",
    "            model.fit(train)\n",
    "            future = model.make_future_dataframe(periods=len(test), freq='W')\n",
    "            forecast = model.predict(future)\n",
    "            forecast = forecast['yhat'][-len(test):].values\n",
    "            return forecast\n",
    "        \n",
    "        for i in timeseries_key_values:\n",
    "            df_prod1 = df1[df1['TimeSeries_Key']==i]\n",
    "    \n",
    "            print(\"----------RESULT FOR\",i,\"----------\")\n",
    "\n",
    "            # Convert 'calweek' column to datetime format\n",
    "            df_prod1['calweek'] = pd.to_datetime(df_prod1['calweek'].astype(str) + '1', format='%Y%W%w')\n",
    "\n",
    "            # Group data by 'calweek' and aggregate 'Quantity'\n",
    "            ts_data = df_prod1.groupby('calweek')['Quantity'].sum().reset_index()\n",
    "\n",
    "            # Rename columns for Prophet model\n",
    "            ts_data.rename(columns={'calweek': 'ds', 'Quantity': 'y'}, inplace=True)\n",
    "\n",
    "            # Split data into training and testing sets\n",
    "            n_train = int(len(ts_data) * 0.8)  # 80% for training, 20% for testing\n",
    "            train = ts_data.iloc[:n_train]\n",
    "            test = ts_data.iloc[n_train:]\n",
    "\n",
    "            # Perform forecasting using ARIMA\n",
    "            arima_predictions = arima_forecast(train['y'], test['y'])\n",
    "\n",
    "            # Perform forecasting using SARIMA\n",
    "            sarima_predictions = sarima_forecast(train['y'], test['y'])\n",
    "\n",
    "            # Perform forecasting using Holt-Winters\n",
    "            holt_winters_predictions = holt_winters_forecast(train['y'], test['y'])\n",
    "    \n",
    "\n",
    "            # Perform forecasting using Facebook Prophet\n",
    "            prophet_predictions = prophet_forecast(train, test)\n",
    "\n",
    "            # Ensemble forecast combining ARIMA and Prophet\n",
    "            ensemble_forecast = (arima_predictions + prophet_predictions) / 2.0\n",
    "\n",
    "            # Calculate MAPE\n",
    "            def calculate_mape(actual, forecast):\n",
    "                return np.mean(np.abs((actual - forecast) / actual)) * 100\n",
    "\n",
    "            # Calculate MAPE for ARIMA\n",
    "            arima_mape = calculate_mape(test['y'], arima_predictions)\n",
    "\n",
    "            # Calculate MAPE for SARIMA\n",
    "            sarima_mape = calculate_mape(test['y'], sarima_predictions)\n",
    "\n",
    "            # Calculate MAPE for Holt-Winters\n",
    "            holt_winters_mape = calculate_mape(test['y'], holt_winters_predictions)\n",
    "\n",
    "            # Calculate MAPE for Facebook Prophet\n",
    "            prophet_mape = calculate_mape(test['y'], prophet_predictions)\n",
    "\n",
    "            # Calculate MAPE for ensemble forecast\n",
    "            ensemble_mape = calculate_mape(test['y'], ensemble_forecast)\n",
    "\n",
    "\n",
    "            # Print MAPE values\n",
    "            print(\"ARIMA - MAPE: {:.2f}%\".format(arima_mape))\n",
    "            print(\"SARIMA - MAPE: {:.2f}%\".format(sarima_mape))\n",
    "            print(\"Holt-Winters - MAPE: {:.2f}%\".format(holt_winters_mape))\n",
    "            print(\"Facebook Prophet - MAPE: {:.2f}%\".format(prophet_mape))\n",
    "            print(\"Ensemble Forecast - MAPE: {:.2f}%\".format(ensemble_mape))\n",
    "    \n",
    "            #FORECASTINGGG\n",
    "\n",
    "            # Split data into training and testing sets\n",
    "            train = ts_data.iloc[:172]\n",
    "\n",
    "            # Facebook Prophet model\n",
    "            def prophet_forecast1(train):\n",
    "                model = Prophet()\n",
    "                model.fit(train)\n",
    "                future = model.make_future_dataframe(periods=13, freq='W')\n",
    "                forecast = model.predict(future)\n",
    "                forecast = forecast['yhat'][-13:].values\n",
    "                return forecast\n",
    "\n",
    "            # ARIMA model\n",
    "            def arima_forecast1(train):\n",
    "                model = ARIMA(train, order=(1, 0, 0))  # ARIMA(p, d, q)\n",
    "                model_fit = model.fit()\n",
    "                forecast = model_fit.forecast(steps=13)\n",
    "                return forecast\n",
    "    \n",
    "            # Perform forecasting using ARIMA\n",
    "            arima_predictions = arima_forecast1(train['y'])\n",
    "\n",
    "            # Perform forecasting using Facebook Prophet\n",
    "            prophet_predictions = prophet_forecast1(train)\n",
    "\n",
    "            # Ensemble forecast combining ARIMA and Prophet\n",
    "            ensemble_predictions = np.array((arima_predictions + prophet_predictions) / 2.0)\n",
    "            print(ensemble_predictions)\n",
    "\n",
    "            # Generate date values for the forecasted period\n",
    "            start_date = ts_data['ds'].iloc[-1]\n",
    "            forecast_dates = pd.date_range(start=start_date, periods=len(test), freq='W').strftime('%Y-%m-%d')\n",
    "            for j in range(13):\n",
    "                results_df = pd.concat([results_df, pd.DataFrame({\n",
    "                    'date': [forecast_dates[j]],\n",
    "                    'TimeSeries_Key': [i],\n",
    "                    'Ensemble_Predicted': ensemble_predictions[j],\n",
    "                    'Prophet_Predicted': [prophet_predictions[j]]\n",
    "            \n",
    "            })], ignore_index=True)\n",
    "            results_mape = pd.concat([results_mape, pd.DataFrame({\n",
    "                'date': [forecast_dates[j]],\n",
    "                'TimeSeries_Key': [i],\n",
    "                'Prophet_MAPE': [prophet_mape],\n",
    "                'ARIMA_MAPE': [arima_mape],\n",
    "                'SARIMA_MAPE': [sarima_mape],\n",
    "                'HoltWinters_MAPE': [holt_winters_mape],\n",
    "                'Ensemble_MAPE': [ensemble_mape]\n",
    "            })], ignore_index=True) \n",
    "            \n",
    "        #results_df\n",
    "\n",
    "        #results_mape\n",
    "\n",
    "        # Convert the date column to datetime format\n",
    "        results_df['date'] = pd.to_datetime(results_df['date'])\n",
    "\n",
    "        # Convert the date format to 'YYYY-WW'\n",
    "        results_df['calweek'] = results_df['date'].dt.strftime('%Y%U')\n",
    "\n",
    "        merged_df = pd.merge(results_df, combined_df, on='TimeSeries_Key')\n",
    "        merged_df.drop_duplicates(subset=['date', 'TimeSeries_Key'], inplace=True)\n",
    "        merged_df.reset_index(drop=True, inplace=True)\n",
    "        #merged_df\n",
    "\n",
    "        # Select specific columns from each dataframe\n",
    "        final_df = df[['calweek', 'Cluster/Country/Region', 'SubCluster', 'brand', 'dc', 'TimeSeries_Key', 'Quantity']]\n",
    "        final_pred_df = merged_df[['calweek', 'Cluster/Country/Region', 'SubCluster', 'brand', 'dc', 'TimeSeries_Key', 'Ensemble_Predicted']]\n",
    "\n",
    "        # Concatenate the dataframes\n",
    "        final_univariate_csv = pd.concat([final_df, final_pred_df], axis=0)\n",
    "        #final_univariate_csv\n",
    "         \n",
    "        return final_univariate_csv\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error occurred:\", str(e))\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "# Main program\n",
    "if __name__ == '__main__':\n",
    "    # Get the file path from the user via command-line input\n",
    "    file_path = input(\"Enter the path to the Excel file: \")\n",
    "    \n",
    "    # Call the function to process the Excel file\n",
    "    final_csv = pd.DataFrame(process_excel_file(file_path))\n",
    "    \n",
    "    # Save the result as a CSV file\n",
    "    final_csv.to_csv('final_univariate.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
