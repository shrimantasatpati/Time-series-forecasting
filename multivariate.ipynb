{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cea94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pmdarima as pm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from prophet import Prophet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process the Excel file\n",
    "def process_excel_file(file_path):\n",
    "    try:\n",
    "        # Read the Excel file into a pandas DataFrame\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "        \n",
    "        df['TimeSeries_Key'] = df['product_code'] + '_' + df['customer_code']\n",
    "\n",
    "        df['Week'] = df['calweek'].astype(str).str[-2:].astype(int)\n",
    "\n",
    "        #df.columns\n",
    "\n",
    "        df1 = df.drop(['Cluster/Country/Region', 'SubCluster', 'brand', 'dc'], axis = 1)\n",
    "\n",
    "        timeseries_key_values = list(df1['TimeSeries_Key'].unique())\n",
    "\n",
    "        # Convert 'calweek' column to datetime format\n",
    "        df1['calweek'] = pd.to_datetime(df1['calweek'].astype(str) + '1', format='%Y%W%w')\n",
    "\n",
    "        # Generate new columns for year, month, and quarter\n",
    "        df1[\"year\"] = df1[\"calweek\"].dt.year\n",
    "        df1[\"month\"] = df1[\"calweek\"].dt.month\n",
    "        df1[\"quarter\"] = df1[\"calweek\"].dt.quarter\n",
    "        # Generate a new column for season\n",
    "        df1['season'] = df1['calweek'].dt.month.apply(lambda x: (x%12 + 3)//3)\n",
    "        \n",
    "        start_week = 16\n",
    "        end_week = 28\n",
    "\n",
    "        filtered_df = df[(df['Week'] >= start_week) & (df['Week'] <= end_week)]\n",
    "\n",
    "        def generate_dataframes():\n",
    "            grouped_df = filtered_df.groupby('TimeSeries_Key')\n",
    "            for key, group in grouped_df:\n",
    "                group['TimeSeries_Key'] = key\n",
    "                yield group\n",
    "\n",
    "        combined_df = pd.concat(generate_dataframes(), ignore_index=True)\n",
    "        combined_df = combined_df.drop_duplicates(subset=['TimeSeries_Key', 'Week'])\n",
    "        combined_df = combined_df.drop([\"product_code\", \"customer_code\", \"Quantity\", \"Week\", \"calweek\"], axis = 1)\n",
    "        #combined_df\n",
    "        \n",
    "        df_prod1 = df1[df1['TimeSeries_Key']=='PRDCT1_CUSTA']\n",
    "        #df_prod1\n",
    "        # Group data by 'calweek' and aggregate 'Quantity'\n",
    "        ts_data = df_prod1.groupby('calweek')['Quantity'].sum()\n",
    "        #ts_data\n",
    "        \n",
    "#         from statsmodels.tsa.stattools import adfuller\n",
    "#         passing_data=adfuller(ts_data)\n",
    "#         def adf_test(sales):\n",
    "#             result=adfuller(ts_data)\n",
    "#             labels = ['Test parameters', 'p-value','#Lags Used','Dataset observations']\n",
    "#             for value,label in zip(result,labels):\n",
    "#                 print(label+' : '+str(value) )\n",
    "#             if result[1] <= 0.05:\n",
    "#                 print(\"Dataset is stationary\")\n",
    "#             else:\n",
    "#                 print(\"Dataset is non-stationary \")\n",
    "#         adf_test(ts_data)\n",
    "\n",
    "        # Create an empty dataframe to store MAPE values\n",
    "        results_df = pd.DataFrame()\n",
    "        results_mape = pd.DataFrame()\n",
    "        \n",
    "        for i in timeseries_key_values:\n",
    "            df_prod1 = df1[df1['TimeSeries_Key']==i]\n",
    "\n",
    "            print(\"----------RESULT FOR\",i,\"----------\")\n",
    "\n",
    "            window_size = 3  # Define the window size for rolling average\n",
    "\n",
    "            # Calculate rolling average for Quantity column\n",
    "            df_prod1['Rolling_Average'] = df_prod1['Quantity'].rolling(window=window_size).mean()\n",
    "            # Lagged Quantity\n",
    "            df_prod1['lag1_Quantity'] = df_prod1['Quantity'].shift(1)\n",
    "\n",
    "            # Split data into training and testing sets\n",
    "            train_size = int(len(df1) * 0.8)  # Adjust the split ratio as per your requirement\n",
    "            train_df = df1[:train_size]\n",
    "            test_df = df1[train_size:]\n",
    "\n",
    "            # Select the relevant columns for modeling\n",
    "            X_train = train_df.drop(['calweek', 'product_code', 'customer_code', 'TimeSeries_Key', 'Quantity', 'season','Week'], axis=1)\n",
    "            y_train = train_df['Quantity']\n",
    "            X_test = test_df.drop(['calweek', 'product_code', 'customer_code', 'TimeSeries_Key', 'Quantity', 'season','Week'], axis=1)\n",
    "            y_test = test_df['Quantity']\n",
    "\n",
    "            # Find optimal p, d, q values using Auto ARIMA\n",
    "            auto_arima_model = pm.auto_arima(y_train, seasonal=False, suppress_warnings=True)\n",
    "            p, d, q = auto_arima_model.order\n",
    "\n",
    "            # ARIMA model\n",
    "            arima_model = ARIMA(endog=y_train, order=(p, d, q))  # ARIMA(p, d, q)\n",
    "            arima_fit = arima_model.fit()\n",
    "            arima_predictions = arima_fit.forecast(steps=len(X_test))\n",
    "\n",
    "            # SARIMAX model\n",
    "            sarimax_model = SARIMAX(endog=y_train, exog=X_train, order=(p, d, q))  # SARIMAX(p, d, q)\n",
    "            sarimax_fit = sarimax_model.fit()\n",
    "            sarimax_predictions = sarimax_fit.forecast(steps=len(X_test), exog=X_test)\n",
    "\n",
    "            # Random Forest\n",
    "            rf = RandomForestRegressor(n_estimators=100)\n",
    "            rf.fit(X_train, y_train)\n",
    "            rf_predictions = rf.predict(X_test)\n",
    "\n",
    "            #ExtraTrees \n",
    "            et_extra_model = ExtraTreesRegressor(n_estimators=100)\n",
    "            et_extra_model.fit(X_train, y_train)\n",
    "            et_extra_predictions = et_extra_model.predict(X_test)\n",
    "\n",
    "            # Gradient Boosting with XGBoost\n",
    "            xgb_model = XGBRegressor(n_estimators=100)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "            xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "            # Calculate MAPE\n",
    "            def calculate_mape(actual, forecast):\n",
    "                return np.mean(np.abs((actual - forecast) / actual)) * 100\n",
    "\n",
    "            # Calculate MAPE for ARIMA\n",
    "            arima_mape = calculate_mape(test_df['Quantity'], arima_predictions)\n",
    "\n",
    "            # Calculate MAPE for SARIMA\n",
    "            sarima_mape = calculate_mape(test_df['Quantity'], sarimax_predictions)\n",
    "\n",
    "            # Calculate MAPE for RandomForest\n",
    "            rf_mape = calculate_mape(test_df['Quantity'], rf_predictions)\n",
    "\n",
    "            # Calculate MAPE for ExtraTrees\n",
    "            et_extra_mape = calculate_mape(test_df['Quantity'], et_extra_predictions)\n",
    "\n",
    "             # Calculate MAPE for Gradient Boosting with XGBoost\n",
    "            xgb_mape = calculate_mape(test_df['Quantity'], xgb_predictions)    \n",
    "\n",
    "\n",
    "            #Print MAPE values \n",
    "            print(\"ARIMA - MAPE: {:.2f}%\".format(arima_mape))\n",
    "            print(\"SARIMA - MAPE: {:.2f}%\".format(sarima_mape))\n",
    "            print(\"RANDOMFOREST - MAPE: {:.2f}%\".format(rf_mape))\n",
    "            print(\"ExtraTrees - MAPE: {:.2f}%\".format(et_extra_mape))\n",
    "            print(\"XGB - MAPE: {:.2f}%\".format(xgb_mape))\n",
    "\n",
    "             #FORECASTINGGG\n",
    "\n",
    "            # Split data into training and testing sets\n",
    "            train = df_prod1\n",
    "            y_train = train['Quantity']\n",
    "\n",
    "            # ARIMA model\n",
    "            arima_model = ARIMA(endog=y_train, order=(1, 0, 0))  # ARIMA(p, d, q)\n",
    "            arima_fit = arima_model.fit()\n",
    "            arima_predictions = arima_fit.forecast(steps=13)\n",
    "\n",
    "            #print(arima_predictions)\n",
    "            # Convert arima_predictions to a list\n",
    "            arima_predictions = arima_predictions.tolist()\n",
    "\n",
    "            # Generate date values for the forecasted period\n",
    "            forecast_dates = pd.date_range(start=df_prod1['calweek'].iloc[-1], periods=13, freq='W').strftime('%Y-%m-%d')\n",
    "\n",
    "            for j in range(13):\n",
    "                results_df = pd.concat([results_df, pd.DataFrame({\n",
    "                    'date': [forecast_dates[j]],\n",
    "                    'TimeSeries_Key': [i],\n",
    "                    'ARIMA_Predicted': [arima_predictions[j]]\n",
    "                     })], ignore_index=True)\n",
    "\n",
    "                results_mape = pd.concat([results_mape, pd.DataFrame({\n",
    "                    'date': [forecast_dates[j]],\n",
    "                    'TimeSeries_Key': [i],\n",
    "                    'ARIMA_MAPE': [arima_mape],\n",
    "                    'SARIMA_MAPE': [sarima_mape],\n",
    "                    'RandomForest_MAPE': [rf_mape],\n",
    "                    'ExtraTrees_MAPE': [et_extra_mape],\n",
    "                    'XGB_MAPE': [xgb_mape],\n",
    "                })], ignore_index=True) \n",
    "                \n",
    "        #results_df\n",
    "\n",
    "        #results_mape\n",
    "\n",
    "        # Convert the date column to datetime format\n",
    "        results_df['date'] = pd.to_datetime(results_df['date'])\n",
    "\n",
    "        # Convert the date format to 'YYYY-WW'\n",
    "        results_df['calweek'] = results_df['date'].dt.strftime('%Y%U')\n",
    "\n",
    "        merged_df = pd.merge(results_df, combined_df, on='TimeSeries_Key')\n",
    "        merged_df.drop_duplicates(subset=['date', 'TimeSeries_Key'], inplace=True)\n",
    "        merged_df.reset_index(drop=True, inplace=True)\n",
    "        #merged_df\n",
    "\n",
    "        # Select specific columns from each dataframe\n",
    "        final_df = df[['calweek', 'Cluster/Country/Region', 'SubCluster', 'brand', 'dc', 'TimeSeries_Key', 'Quantity']]\n",
    "        final_pred_df = merged_df[['calweek', 'Cluster/Country/Region', 'SubCluster', 'brand', 'dc', 'TimeSeries_Key', 'ARIMA_Predicted']]\n",
    "\n",
    "        # Concatenate the dataframes\n",
    "        final_multivariate_csv = pd.concat([final_df, final_pred_df], axis=0)\n",
    "        #final_multivariate_csv\n",
    "         \n",
    "        return final_multivariate_csv\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error occurred:\", str(e))\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "# Main program\n",
    "if __name__ == '__main__':\n",
    "    # Get the file path from the user via command-line input\n",
    "    file_path = input(\"Enter the path to the Excel file: \")\n",
    "    \n",
    "    # Call the function to process the Excel file\n",
    "    final_csv = pd.DataFrame(process_excel_file(file_path))\n",
    "    \n",
    "    # Save the result as a CSV file\n",
    "    final_csv.to_csv('final_multivariate.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
